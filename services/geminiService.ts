
import { GoogleGenAI } from "@google/genai";
import { AnalysisType, PromptConfig } from "../types";

// Initialize the client
const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

const MODEL_NAME = "gemini-3-pro-preview";

// Token Safety Limits
const SAFE_CHUNK_SIZE = 400000; 
const CHUNK_OVERLAP = 2000;

// --- Default Prompts Exported for UI ---

export const DEFAULT_PROMPTS: Record<AnalysisType, PromptConfig> = {
  outline: {
    system: `ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç½‘æ–‡ä¸»ç¼–å’Œå‰§æƒ…æ¶æ„å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æå°è¯´æ–‡æœ¬ï¼Œæå–æå…·æ·±åº¦çš„ç»“æ„åŒ–å¤§çº²ã€‚
è¯·å¿½ç•¥æ’ç‰ˆå¹²æ‰°ã€‚é‡ç‚¹å…³æ³¨ï¼šæ•…äº‹æ¨è¿›ã€å†²çªå‡çº§ã€é«˜æ½®èŠ‚ç‚¹ã€‚`,
    user: `è¯·åˆ†æè¿™æ®µå°è¯´æ–‡æœ¬ã€‚

**è¾“å‡ºè¦æ±‚ï¼ˆMarkdownï¼‰**ï¼š
1. **æœ¬æ®µå‰§æƒ…æ¦‚æ‹¬**ï¼šç”¨ä¸€å¥è¯æ€»ç»“è¿™éƒ¨åˆ†è®²äº†ä»€ä¹ˆã€‚
2. **è¯¦ç»†ç« çº²/äº‹ä»¶æµ**ï¼š
   - æŒ‰æƒ…èŠ‚å‘ç”Ÿé¡ºåºï¼Œåˆ—å‡ºå…³é”®äº‹ä»¶ã€‚
   - æ ‡æ³¨ã€é«˜æ½®ã€‘ã€ã€è½¬æŠ˜ã€‘ã€ã€ä¼ç¬”ã€‘ç­‰æ ‡ç­¾ã€‚
   - å¦‚æœè¿™éƒ¨åˆ†åŒ…å«å…·ä½“çš„ç« èŠ‚åˆ’åˆ†ï¼ˆå¦‚ç¬¬Xç« ï¼‰ï¼Œè¯·æ˜ç¡®åˆ—å‡ºç« èŠ‚æ ‡é¢˜ã€‚

æ³¨æ„ï¼šè¯·ä¿æŒå®¢è§‚ã€ç²¾ç‚¼ã€‚`
  },
  style: {
    system: `ä½ æ˜¯ä¸€ä½æ¯’èˆŒåˆä¸“ä¸šçš„æ–‡å­¦è¯„è®ºå®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å‰–æå°è¯´çš„â€œæ–‡é£â€ä¸â€œéª¨ç›¸â€ã€‚
å…³æ³¨ï¼šå™äº‹è§†è§’ã€ç”¨è¯ä¹ æƒ¯ã€æƒ…æ„Ÿå¯†åº¦ã€äººç‰©å¯¹è¯é£æ ¼ã€‚`,
    user: `è¯·åŸºäºè¿™ä»½æ–‡æœ¬æ ·æœ¬è¿›è¡Œå…¨ä¹¦å†™ä½œé£æ ¼çš„æ·±åº¦è¯„æµ‹ã€‚

**åˆ†æç»´åº¦**ï¼š
1. **å™äº‹èŠ‚å¥**ï¼šæ˜¯å¿«èŠ‚å¥çˆ½æ–‡ï¼Œè¿˜æ˜¯æ…¢çƒ­é“ºå«ï¼Ÿ
2. **è¯­è¨€ç‰¹è‰²**ï¼šè¯·æ‘˜å½•1-2ä¸ªä¾‹å¥è¿›è¡Œç‚¹è¯„ï¼ˆå¦‚ï¼šåä¸½å †ç Œã€å¹²ç»ƒç™½æã€å¹½é»˜ç©æ¢—ï¼‰ã€‚
3. **äººç‰©åˆ»ç”»**ï¼šä½œè€…æ“…é•¿é€šè¿‡ä»€ä¹ˆæ–¹å¼ç«‹äººè®¾ï¼Ÿ
4. **æƒ…æ„ŸåŸºè°ƒ**ï¼šè¯»èµ·æ¥çš„æ„Ÿè§‰ï¼ˆçƒ­è¡€ã€å‹æŠ‘ã€æ¸©é¦¨ã€æ‚¬ç–‘ï¼‰ã€‚
5. **ä¸»ç¼–ç‚¹è¯„**ï¼šå®¢è§‚è¯„ä»·å…¶ä¼˜ç¼ºç‚¹ã€‚`
  },
  settings: {
    system: `ä½ æ˜¯ä¸€ä½å¥‡å¹»/ç§‘å¹»è®¾å®šé›†ç¼–çº‚è€…ã€‚ä½ çš„ä»»åŠ¡æ˜¯æŒ–æ˜æ–‡æœ¬ä¸­éšå«çš„ä¸–ç•Œè§‚è®¾å®šã€‚
å…³æ³¨ï¼šåœ°å›¾åœ°ç†ã€åŠ›é‡/ä¿®ç‚¼ä½“ç³»ã€åŠ¿åŠ›æ¶æ„ã€ä¸“æœ‰åè¯ã€‚`,
    user: `è¯·æå–è¿™æ®µæ–‡æœ¬ä¸­å‡ºç°çš„æ‰€æœ‰æ–°è®¾å®šã€‚

**è¯·ç»“æ„åŒ–æ•´ç†ä»¥ä¸‹å†…å®¹ï¼ˆè‹¥æœ‰ï¼‰**ï¼š
- **åœ°ç†ä¸åŠ¿åŠ›**ï¼šå›½å®¶ã€å®—é—¨ã€åŸå¸‚ã€ç‰¹æ®Šåœ°å½¢ã€‚
- **åŠ›é‡ä½“ç³»**ï¼šå¢ƒç•Œåˆ’åˆ†ã€ç‰¹æ®Šèƒ½åŠ›ã€æ­¦å™¨é“å…·ã€‚
- **äººç‰©å…³ç³»**ï¼šæ–°ç™»åœºçš„é‡è¦äººç‰©åŠå…¶èº«ä»½ã€‚
- **ä¸“æœ‰åè¯**ï¼šç‹¬ç‰¹çš„æœ¯è¯­è§£é‡Šã€‚

å¦‚æœè¿™æ®µæ–‡æœ¬æ²¡æœ‰æ–°è®¾å®šï¼Œè¯·ç®€çŸ­è¯´æ˜ã€‚`
  }
};

/**
 * Splits text into overlapping chunks
 */
const createChunks = (text: string): string[] => {
  if (text.length <= SAFE_CHUNK_SIZE) return [text];
  
  const chunks: string[] = [];
  let startIndex = 0;
  
  while (startIndex < text.length) {
    const endIndex = Math.min(startIndex + SAFE_CHUNK_SIZE, text.length);
    chunks.push(text.slice(startIndex, endIndex));
    
    if (endIndex === text.length) break;
    startIndex += (SAFE_CHUNK_SIZE - CHUNK_OVERLAP);
  }
  
  return chunks;
};

/**
 * Creates a sampled version of the text for global style analysis
 */
const sampleTextForStyle = (text: string): string => {
  if (text.length <= SAFE_CHUNK_SIZE) return text;

  const sliceSize = 150000; 
  const start = text.slice(0, sliceSize);
  
  const midIndex = Math.floor(text.length / 2) - Math.floor(sliceSize / 2);
  const mid = text.slice(midIndex, midIndex + sliceSize);
  
  const end = text.slice(text.length - sliceSize);
  
  return `${start}\n\n...[æ­¤å¤„çœç•¥ä¸­é—´å†…å®¹]...\n\n${mid}\n\n...[æ­¤å¤„çœç•¥ä¸­é—´å†…å®¹]...\n\n${end}`;
};

/**
 * Generates the specific user prompt for a chunk by appending context to the user's custom prompt
 */
const formatUserPrompt = (basePrompt: string, isPartial: boolean, chunkIndex?: number, total?: number): string => {
  const progressStr = isPartial && total ? `(å½“å‰æ­£åœ¨åˆ†æç¬¬ ${chunkIndex! + 1}/${total} éƒ¨åˆ†)` : "";
  return `${basePrompt}\n\n${progressStr}`;
};

/**
 * Calls Gemini API for a single chunk
 */
const callGemini = async (
  text: string, 
  systemInstruction: string,
  userPrompt: string,
  onStream?: (text: string) => void
): Promise<string> => {
  const responseStream = await ai.models.generateContentStream({
    model: MODEL_NAME,
    contents: [
      {
        role: 'user',
        parts: [
          { text: userPrompt },
          { text: `\n\n--- å¾…åˆ†ææ–‡æœ¬ ---\n\n${text}` }
        ]
      }
    ],
    config: {
      systemInstruction: systemInstruction,
      thinkingConfig: {
        thinkingBudget: 1024, 
      },
    }
  });

  let fullText = "";
  for await (const chunk of responseStream) {
    const chunkText = chunk.text;
    if (chunkText) {
      fullText += chunkText;
      if (onStream) onStream(chunkText);
    }
  }
  return fullText;
};

/**
 * Main analysis function
 */
export const analyzeNovelText = async (
  text: string, 
  type: AnalysisType,
  promptConfig: PromptConfig,
  onStream?: (chunkText: string) => void
): Promise<string> => {
  try {
    let accumulatedResult = "";
    
    const handleStream = (newContent: string) => {
      accumulatedResult += newContent;
      if (onStream) {
        onStream(accumulatedResult);
      }
    };

    // STRATEGY 1: Style Analysis (Sampling)
    if (type === 'style') {
      const sampledText = sampleTextForStyle(text);
      const prompt = formatUserPrompt(promptConfig.user, false);
      
      if (text.length > SAFE_CHUNK_SIZE) {
         handleStream(`*æ³¨ï¼šç”±äºæ–‡ä»¶è¿‡å¤§ï¼Œå·²è‡ªåŠ¨æˆªå–ã€å¼€å¤´ã€‘ã€ã€ä¸­é—´ã€‘ã€ã€ç»“å°¾ã€‘ä¸‰éƒ¨åˆ†æ ·æœ¬è¿›è¡Œç»¼åˆé£æ ¼åˆ†æ...*\n\n---\n\n`);
      }
      
      await callGemini(sampledText, promptConfig.system, prompt, (chunk) => handleStream(chunk));
      return accumulatedResult;
    }

    // STRATEGY 2: Sequential Chunking (Outline & Settings)
    const chunks = createChunks(text);
    
    if (chunks.length === 1) {
      const prompt = formatUserPrompt(promptConfig.user, false);
      await callGemini(chunks[0], promptConfig.system, prompt, (chunk) => handleStream(chunk));
    } else {
      handleStream(`*æ£€æµ‹åˆ°è¶…é•¿æ–‡æœ¬ (${chunks.length} ä¸ªéƒ¨åˆ†)ï¼Œæ­£åœ¨åˆ†æ®µæ·±åº¦åˆ†æä¸­...*\n\n`);
      
      for (let i = 0; i < chunks.length; i++) {
        const header = `\n\n### ğŸ“œ ç¬¬ ${i + 1} éƒ¨åˆ†åˆ†æ (å…± ${chunks.length} éƒ¨åˆ†)\n\n`;
        handleStream(header);
        
        const prompt = formatUserPrompt(promptConfig.user, true, i, chunks.length);
        
        await callGemini(chunks[i], promptConfig.system, prompt, (chunk) => handleStream(chunk));
        
        handleStream(`\n\n---\n`);
      }
    }

    return accumulatedResult;

  } catch (error) {
    console.error(`Error analyzing ${type}:`, error);
    if (error instanceof Error && error.message.includes("token")) {
        throw new Error("æ–‡æœ¬è¿‡é•¿ï¼Œå°½ç®¡å·²å°è¯•åˆ†ç‰‡ï¼Œå•ç‰‡å†…å®¹ä»è¶…è¿‡æ¨¡å‹é™åˆ¶ã€‚å»ºè®®æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«å¤§é‡éæ–‡æœ¬å­—ç¬¦ã€‚");
    }
    throw error;
  }
};
