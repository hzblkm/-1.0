
import { GoogleGenAI } from "@google/genai";
import { AnalysisType, PromptConfig } from "../types";

const MODEL_NAME = "gemini-3-pro-preview";

// Token/Char Limits
// Reduced to 50k chars for better stability and instruction following.
// 300k was causing hallucinations and timeouts.
const TARGET_CHUNK_SIZE = 50000; 
const MIN_CHUNK_SIZE = 5000; 

// --- Default Prompts Exported for UI ---

export const DEFAULT_PROMPTS: Record<AnalysisType, PromptConfig> = {
  summary: {
    system: `ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„ç½‘æ–‡ä¸»ç¼–ã€‚ä½ çš„ä»»åŠ¡æ˜¯é€šè¯»å°è¯´åŸç¨¿ï¼Œæ•´ç†å‡ºä¸€ä»½**è¯¦å°½çš„å‰§æƒ…æ¢—æ¦‚**ã€‚
**æ ¸å¿ƒåŸåˆ™**ï¼š
1. **ä¸è¦è¿‡åº¦å‹ç¼©**ï¼š5ä¸‡å­—çš„æ–‡æœ¬åŒ…å«å¤§é‡ç»†èŠ‚ï¼Œè¯·ä¸è¦åªå†™ä¸€ä¸¤å¥è¯ã€‚æˆ‘éœ€è¦çŸ¥é“å…·ä½“å‘ç”Ÿäº†ä»€ä¹ˆã€‚
2. **ä¿ç•™äº‹ä»¶é€»è¾‘**ï¼šèµ·å› ã€ç»è¿‡ã€ç»“æœè¦å®Œæ•´ã€‚
3. **å…³é”®ä¿¡æ¯ä¸é—æ¼**ï¼šæ–°å‡ºåœºçš„äººç‰©å§“åã€è·å¾—çš„ç‰©å“/åŠŸæ³•ã€åœ°åã€ç­‰çº§å˜åŒ–ç­‰å¿…é¡»è®°å½•ã€‚
4. **å®¢è§‚é™ˆè¿°**ï¼šåªé™ˆè¿°å‰§æƒ…ï¼Œä¸è¦å‘è¡¨è¯„è®ºã€‚`,
    user: `è¯·é˜…è¯»è¿™æ®µå°è¯´æ–‡æœ¬ï¼ˆçº¦5ä¸‡å­—ç¬¦ï¼‰ï¼Œå¹¶ç”Ÿæˆä¸€ä»½**è¯¦ç»†çš„äº‹ä»¶æµæ°´è´¦**ã€‚

**è¾“å‡ºè¦æ±‚**ï¼š
1. **åˆ†åœºæ™¯/åˆ†äº‹ä»¶å™è¿°**ï¼šå¦‚æœè¿™æ®µæ–‡æœ¬è·¨è¶Šäº†å¤šä¸ªåœºæ™¯ï¼ˆå¦‚ï¼šå…ˆåœ¨å®¶é‡Œä¿®ç‚¼ï¼Œç„¶åå»æ‹å–è¡Œï¼Œæœ€ååœ¨é‡å¤–æ‰“æ¶ï¼‰ï¼Œè¯·åˆ†å¼€æ®µè½æè¿°ã€‚
2. **åŒ…å«å¯¹è¯é‡ç‚¹**ï¼šå¦‚æœæœ‰å…³é”®çš„å‰§æƒ…å¯¹è¯ï¼Œè¯·æ¦‚æ‹¬å¯¹è¯çš„æ ¸å¿ƒå†…å®¹ï¼ˆå¦‚â€œAå¨èƒBäº¤å‡ºå®ç‰©ï¼ŒBæ‹’ç»å¹¶æå‡ºå†³æ–—â€ï¼‰ã€‚
3. **æˆ˜æ–—/å†²çªç»†èŠ‚**ï¼šå¦‚æœæ˜¯æˆ˜æ–—æƒ…èŠ‚ï¼Œç®€è¿°åŒæ–¹ä½¿ç”¨çš„æ‹›å¼å’Œèƒœè´Ÿè¿‡ç¨‹ã€‚
4. **é•¿åº¦é€‚ä¸­**ï¼šè¯·è¾“å‡ºçº¦ 500-1000 å­—çš„è¯¦ç»†æ‘˜è¦ï¼Œç¡®ä¿æˆ‘çœ‹æ‘˜è¦å°±èƒ½å®Œå…¨æ˜ç™½è¿™æ®µå†™äº†ä»€ä¹ˆã€‚

ï¼ˆè¯·å¼€å§‹åˆ†æ...ï¼‰`
  },
  outline: {
    system: `ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç½‘æ–‡ä¸»ç¼–å’Œå‰§æƒ…æ¶æ„å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æå°è¯´æ–‡æœ¬ï¼Œæå–æå…·æ·±åº¦çš„ç»“æ„åŒ–å¤§çº²ã€‚
**æ ¸å¿ƒåŸåˆ™ï¼šä¸¥ç¦ç¼–é€ ã€‚åªåˆ†ææä¾›çš„æ–‡æœ¬å†…å®¹ã€‚**
è¯·å¿½ç•¥æ’ç‰ˆå¹²æ‰°ã€‚é‡ç‚¹å…³æ³¨ï¼šæ•…äº‹æ¨è¿›ã€å†²çªå‡çº§ã€é«˜æ½®èŠ‚ç‚¹ã€‚`,
    user: `è¯·åˆ†æè¿™æ®µå°è¯´æ–‡æœ¬ï¼ˆæˆ–å‰§æƒ…æ‘˜è¦ï¼‰ã€‚

**è¾“å‡ºè¦æ±‚ï¼ˆMarkdownï¼‰**ï¼š
1. **æœ¬æ®µå‰§æƒ…æ¦‚æ‹¬**ï¼šç”¨ä¸€å¥è¯æ€»ç»“è¿™éƒ¨åˆ†è®²äº†ä»€ä¹ˆã€‚
2. **è¯¦ç»†ç« çº²/äº‹ä»¶æµ**ï¼š
   - æŒ‰æƒ…èŠ‚å‘ç”Ÿé¡ºåºï¼Œåˆ—å‡ºå…³é”®äº‹ä»¶ã€‚
   - æ ‡æ³¨ã€é«˜æ½®ã€‘ã€ã€è½¬æŠ˜ã€‘ã€ã€ä¼ç¬”ã€‘ç­‰æ ‡ç­¾ã€‚
   - å¦‚æœè¿™éƒ¨åˆ†åŒ…å«å…·ä½“çš„ç« èŠ‚åˆ’åˆ†ï¼ˆå¦‚ç¬¬Xç« ï¼‰ï¼Œè¯·æ˜ç¡®åˆ—å‡ºç« èŠ‚æ ‡é¢˜ã€‚

æ³¨æ„ï¼šè¯·ä¿æŒå®¢è§‚ã€ç²¾ç‚¼ã€‚`
  },
  style: {
    system: `ä½ æ˜¯ä¸€ä½æ¯’èˆŒåˆä¸“ä¸šçš„æ–‡å­¦è¯„è®ºå®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å‰–æå°è¯´çš„â€œæ–‡é£â€ä¸â€œéª¨ç›¸â€ã€‚
å…³æ³¨ï¼šå™äº‹è§†è§’ã€ç”¨è¯ä¹ æƒ¯ã€æƒ…æ„Ÿå¯†åº¦ã€äººç‰©å¯¹è¯é£æ ¼ã€‚`,
    user: `è¯·åŸºäºè¿™ä»½æ–‡æœ¬æ ·æœ¬è¿›è¡Œå…¨ä¹¦å†™ä½œé£æ ¼çš„æ·±åº¦è¯„æµ‹ã€‚

**åˆ†æç»´åº¦**ï¼š
1. **å™äº‹èŠ‚å¥**ï¼šæ˜¯å¿«èŠ‚å¥çˆ½æ–‡ï¼Œè¿˜æ˜¯æ…¢çƒ­é“ºå«ï¼Ÿ
2. **è¯­è¨€ç‰¹è‰²**ï¼šè¯·æ‘˜å½•1-2ä¸ªä¾‹å¥è¿›è¡Œç‚¹è¯„ï¼ˆå¦‚ï¼šåä¸½å †ç Œã€å¹²ç»ƒç™½æã€å¹½é»˜ç©æ¢—ï¼‰ã€‚
3. **äººç‰©åˆ»ç”»**ï¼šä½œè€…æ“…é•¿é€šè¿‡ä»€ä¹ˆæ–¹å¼ç«‹äººè®¾ï¼Ÿ
4. **æƒ…æ„ŸåŸºè°ƒ**ï¼šè¯»èµ·æ¥çš„æ„Ÿè§‰ï¼ˆçƒ­è¡€ã€å‹æŠ‘ã€æ¸©é¦¨ã€æ‚¬ç–‘ï¼‰ã€‚
5. **ä¸»ç¼–ç‚¹è¯„**ï¼šå®¢è§‚è¯„ä»·å…¶ä¼˜ç¼ºç‚¹ã€‚`
  },
  settings: {
    system: `ä½ æ˜¯ä¸€ä½å¥‡å¹»/ç§‘å¹»è®¾å®šé›†ç¼–çº‚è€…ã€‚ä½ çš„ä»»åŠ¡æ˜¯æŒ–æ˜æ–‡æœ¬ä¸­éšå«çš„ä¸–ç•Œè§‚è®¾å®šã€‚
**æ ¸å¿ƒåŸåˆ™ï¼šä¸¥ç¦ç¼–é€ ã€‚åªæå–æ–‡æœ¬ä¸­æ˜ç¡®æåˆ°çš„è®¾å®šã€‚**
å…³æ³¨ï¼šåœ°å›¾åœ°ç†ã€åŠ›é‡/ä¿®ç‚¼ä½“ç³»ã€åŠ¿åŠ›æ¶æ„ã€ä¸“æœ‰åè¯ã€‚`,
    user: `è¯·æå–è¿™æ®µæ–‡æœ¬ä¸­å‡ºç°çš„æ‰€æœ‰æ–°è®¾å®šã€‚

**è¯·ç»“æ„åŒ–æ•´ç†ä»¥ä¸‹å†…å®¹ï¼ˆè‹¥æœ‰ï¼‰**ï¼š
1. **åœ°ç†ä¸åŠ¿åŠ›**ï¼šå›½å®¶ã€å®—é—¨ã€åŸå¸‚ã€ç‰¹æ®Šåœ°å½¢ã€‚
2. **åŠ›é‡ä½“ç³»**ï¼šå¢ƒç•Œåˆ’åˆ†ã€ç‰¹æ®Šèƒ½åŠ›ã€æ­¦å™¨é“å…·ã€‚
3. **ä¸“æœ‰åè¯**ï¼šç‹¬ç‰¹çš„æœ¯è¯­è§£é‡Šã€‚

å¦‚æœè¿™æ®µæ–‡æœ¬æ²¡æœ‰æ–°è®¾å®šï¼Œè¯·ç®€çŸ­è¯´æ˜ã€‚`
  },
  relationships: {
    system: `ä½ æ˜¯ä¸€ä½èµ„æ·±çš„äººç‰©å¿ƒç†åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯æ¢³ç†å°è¯´ä¸­é”™ç»¼å¤æ‚çš„äººç‰©å…³ç³»ç½‘ã€‚
**æ ¸å¿ƒåŸåˆ™ï¼šä¸¥ç¦ç¼–é€ ã€‚åªåˆ†ææ–‡æœ¬ä¸­å®é™…äº’åŠ¨çš„è§’è‰²ã€‚**
å…³æ³¨ï¼šè§’è‰²é—´çš„äº’åŠ¨ã€æƒ…æ„Ÿå˜åŒ–ã€é˜µè¥å½’å±ã€éšè—çš„ç¾ç»Šã€‚`,
    user: `è¯·åˆ†æè¿™æ®µæ–‡æœ¬ä¸­å‡ºç°çš„äººç‰©åŠå…¶äº’åŠ¨å…³ç³»ã€‚

**è¾“å‡ºè¦æ±‚**ï¼š
1. **ç™»åœºäººç‰©æ¸…å•**ï¼šåˆ—å‡ºæœ¬æ®µå‡ºåœºçš„ä¸»è¦è§’è‰²ã€‚
2. **å…³ç³»åŠ¨æ€**ï¼š
   - [è§’è‰²A] vs [è§’è‰²B]ï¼šæè¿°ä»–ä»¬å½“å‰çš„äº’åŠ¨æ¨¡å¼ï¼ˆå¦‚ï¼šæ•Œå¯¹ã€åˆ©ç”¨ã€æš§æ˜§ã€å¸ˆå¾’ï¼‰ã€‚
   - æ˜¯å¦æœ‰å…³ç³»æ€§è´¨çš„é‡å¤§è½¬æŠ˜ï¼Ÿ
3. **æ½œåœ¨ä¼ç¬”**ï¼šäººç‰©è¡Œä¸ºä¸­æ˜¯å¦æœ‰ä¸åˆå¸¸ç†ã€æš—ç¤ºåç»­å‘å±•çš„ç»†èŠ‚ï¼Ÿ`
  },
  theme: {
    system: `ä½ æ˜¯ä¸€ä½æ–‡å­¦ç³»æ•™æˆã€‚ä½ çš„ä»»åŠ¡æ˜¯é€è¿‡è¡¨é¢çš„æƒ…èŠ‚ï¼Œæç‚¼å°è¯´æ·±å±‚çš„æ¯é¢˜ï¼ˆMotifï¼‰ä¸æ ¸å¿ƒæ€æƒ³ï¼ˆThemeï¼‰ã€‚
å…³æ³¨ï¼šåå¤å‡ºç°çš„æ„è±¡ã€ä¸»è§’çš„é“å¾·å›°å¢ƒã€ä½œè€…æƒ³è¦æ¢è®¨çš„ç¤¾ä¼š/äººæ€§è®®é¢˜ã€‚`,
    user: `è¯·æ·±å…¥è§£è¯»è¿™æ®µæ–‡æœ¬çš„æ·±å±‚å«ä¹‰ã€‚

**åˆ†æç»´åº¦**ï¼š
1. **æ ¸å¿ƒæ¯é¢˜**ï¼šæœ¬æ®µæƒ…èŠ‚åœ¨æ¢è®¨ä»€ä¹ˆï¼Ÿï¼ˆä¾‹å¦‚ï¼šå¤ä»‡çš„ä»£ä»·ã€æˆé•¿çš„é˜µç—›ã€æƒåŠ›çš„å¼‚åŒ–ï¼‰ã€‚
2. **å…³é”®æ„è±¡**ï¼šæ˜¯å¦æœ‰åå¤å‡ºç°çš„è±¡å¾æ€§äº‹ç‰©ï¼Ÿ
3. **ä»·å€¼è§‚å†²çª**ï¼šä¸»è§’åœ¨åšä»€ä¹ˆè‰°éš¾çš„é€‰æ‹©ï¼Ÿè¿™åæ˜ äº†ä»€ä¹ˆä»·å€¼è§‚ï¼Ÿ`
  },
  plotholes: {
    system: `ä½ æ˜¯ä¸€ä½ä»¥â€œæ‰¾èŒ¬â€ä¸ºä¹çš„é€»è¾‘å®¡æŸ¥å‘˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¯»æ‰¾å‰§æƒ…ä¸­çš„ä¸åˆç†ä¹‹å¤„ã€é€»è¾‘æ¼æ´ï¼ˆBugï¼‰å’Œåƒä¹¦è®¾å®šã€‚
å…³æ³¨ï¼šæ—¶é—´çº¿é”™è¯¯ã€æˆ˜åŠ›å´©åã€äººç‰©é™æ™ºã€å‰åè®¾å®šçŸ›ç›¾ã€‚`,
    user: `è¯·ä¸¥æ ¼å®¡æŸ¥è¿™æ®µæ–‡æœ¬çš„é€»è¾‘æ€§ã€‚

**å®¡æŸ¥æŠ¥å‘Š**ï¼š
1. **é€»è¾‘æ¼æ´ï¼ˆå¦‚æœæœ‰ï¼‰**ï¼šæ˜¯å¦æœ‰è§£é‡Šä¸é€šçš„æƒ…èŠ‚ï¼Ÿ
2. **è®¾å®šå†²çªï¼ˆå¦‚æœæœ‰ï¼‰**ï¼šæ˜¯å¦ä¸ä¹‹å‰çš„å·²çŸ¥è®¾å®šï¼ˆå¦‚åŠ›é‡ä½“ç³»ã€äººç‰©æ€§æ ¼ï¼‰çŸ›ç›¾ï¼Ÿ
3. **é™æ™ºè¡Œä¸º**ï¼šè§’è‰²æ˜¯å¦ä¸ºäº†æ¨åŠ¨å‰§æƒ…è€Œå¼ºè¡Œåšå‡ºä¸ç¬¦åˆäººè®¾çš„è ¢äº‹ï¼Ÿ
4. **åˆç†æ€§å»ºè®®**ï¼šå¦‚æœæ˜¯ä½ ï¼Œä½ ä¼šå¦‚ä½•ä¿®æ”¹ä»¥å µä¸Šè¿™ä¸ªæ¼æ´ï¼Ÿ

å¦‚æœæœ¬æ®µé€»è¾‘ä¸¥å¯†ï¼Œè¯·æ³¨æ˜â€œé€»è¾‘é€šé¡ºï¼Œæ— æ˜æ˜¾æ¼æ´â€ã€‚`
  }
};

/**
 * Split text intelligently preserving paragraph/sentence boundaries
 * Exported for UI use
 */
export const createSmartChunks = (text: string): string[] => {
  const chunks: string[] = [];
  let currentPos = 0;

  while (currentPos < text.length) {
    let endPos = currentPos + TARGET_CHUNK_SIZE;
    
    // If remaining text is small enough, take it all
    if (endPos >= text.length) {
      chunks.push(text.slice(currentPos));
      break;
    }

    // Backtrack to find a good breaking point
    // Priority: \n\n (Paragraph) > \n (Line) > ã€‚/./!/? (Sentence)
    let splitPos = -1;
    
    // Search window: look back up to 5k chars from the hard cut limit
    const searchStart = Math.max(currentPos + MIN_CHUNK_SIZE, endPos - 5000);
    const searchEnd = endPos;
    const textWindow = text.slice(searchStart, searchEnd);

    // Helper to map window index to text index
    const toAbsIndex = (windowIndex: number) => searchStart + windowIndex;

    // 1. Try Paragraph break (\n\n)
    const lastParagraph = textWindow.lastIndexOf('\n\n');
    if (lastParagraph !== -1) {
      splitPos = toAbsIndex(lastParagraph) + 2; // Split after the newlines
    }

    // 2. Try Line break (\n)
    if (splitPos === -1) {
        const lastLine = textWindow.lastIndexOf('\n');
        if (lastLine !== -1) {
            splitPos = toAbsIndex(lastLine) + 1;
        }
    }

    // 3. Try Sentence break (Punctuation)
    if (splitPos === -1) {
        // Scan backwards for punctuation
        for (let i = textWindow.length - 1; i >= 0; i--) {
            if (/[ã€‚ï¼ï¼Ÿ\.\!\?]/.test(textWindow[i])) {
                splitPos = toAbsIndex(i) + 1;
                break;
            }
        }
    }

    // 4. Fallback: Hard split at space if possible
    if (splitPos === -1) {
        const lastSpace = textWindow.lastIndexOf(' ');
        if (lastSpace !== -1) {
            splitPos = toAbsIndex(lastSpace) + 1;
        }
    }

    // 5. Ultimate Fallback: Hard cut
    if (splitPos === -1) {
        splitPos = searchEnd;
    }

    chunks.push(text.slice(currentPos, splitPos));
    currentPos = splitPos;
  }
  
  return chunks;
};

/**
 * Creates a smart sampled version of the text for global style analysis
 * Ensures we don't cut in the middle of sentences.
 */
const sampleTextForStyle = (text: string): string => {
  if (text.length <= TARGET_CHUNK_SIZE) return text;

  const SAMPLE_PART_SIZE = 50000; // 50k chars per part for style
  
  // Helper to find a safe boundary forward
  const findSafeEnd = (start: number, length: number) => {
      let target = Math.min(start + length, text.length);
      // Look forward for a bit to find a newline or punctuation
      const lookAheadLimit = Math.min(target + 5000, text.length);
      for (let i = target; i < lookAheadLimit; i++) {
          if (/[\nã€‚ï¼ï¼Ÿ\.\!\?]/.test(text[i])) {
              return i + 1;
          }
      }
      return target; // Fallback
  };

  // Helper to find a safe boundary backward
  const findSafeStart = (target: number) => {
      const lookBackLimit = Math.max(0, target - 5000);
      for (let i = target; i > lookBackLimit; i--) {
          if (/[\nã€‚ï¼ï¼Ÿ\.\!\?]/.test(text[i])) {
              return i + 1;
          }
      }
      return target;
  };

  // 1. Head
  const headEnd = findSafeEnd(0, SAMPLE_PART_SIZE);
  const head = text.slice(0, headEnd);

  // 3. Tail
  const tailTarget = Math.max(0, text.length - SAMPLE_PART_SIZE);
  const tailStart = findSafeStart(tailTarget);
  const tail = text.slice(tailStart);

  // 2. Mid
  const midTarget = Math.floor(text.length / 2) - (SAMPLE_PART_SIZE / 2);
  const midStart = findSafeStart(midTarget);
  const midEnd = findSafeEnd(midStart, SAMPLE_PART_SIZE);
  const mid = text.slice(midStart, midEnd);

  return `${head}\n\n...[æ­¤å¤„çœç•¥ ${((midStart - headEnd)/1000).toFixed(1)}k å­—]...\n\n${mid}\n\n...[æ­¤å¤„çœç•¥ ${((tailStart - midEnd)/1000).toFixed(1)}k å­—]...\n\n${tail}`;
};

/**
 * Generates the specific user prompt for a chunk
 */
const formatUserPrompt = (basePrompt: string, isPartial: boolean, chunkIndex?: number, total?: number): string => {
  const progressStr = isPartial && total ? `(å½“å‰æ­£åœ¨åˆ†æç¬¬ ${chunkIndex! + 1}/${total} éƒ¨åˆ†)` : "";
  return `${basePrompt}\n\n${progressStr}`;
};

/**
 * Calls Gemini API for a single chunk
 */
const callGemini = async (
  text: string, 
  systemInstruction: string,
  userPrompt: string,
  onStream?: (text: string) => void,
  options?: { useThinking?: boolean }
): Promise<string> => {
  // Create a new instance for every call to ensure the latest API Key is used
  const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
  
  // Combine user prompt and text into ONE single text part to avoid context loss.
  // We use strict delimiters to help the model distinguish instructions from content.
  const combinedUserMessage = `${userPrompt}\n\n========== å¾…åˆ†ææ–‡æœ¬å¼€å§‹ ==========\n\n${text}\n\n========== å¾…åˆ†ææ–‡æœ¬ç»“æŸ ==========\n\nè¯·ä¸¥æ ¼åŸºäºä¸Šè¿°ã€å¾…åˆ†ææ–‡æœ¬ã€‘è¿›è¡Œå›ç­”ï¼Œä¸è¦ç¼–é€ ã€‚`;

  console.log(`Calling Gemini. Prompt Length: ${userPrompt.length}, Text Length: ${text.length}, Total: ${combinedUserMessage.length}`);

  // Disable thinking for summaries to prevent over-abstraction
  const thinkingBudget = options?.useThinking === false ? 0 : 32768;

  const responseStream = await ai.models.generateContentStream({
    model: MODEL_NAME,
    contents: [
      {
        role: 'user',
        parts: [
          { text: combinedUserMessage }
        ]
      }
    ],
    config: {
      systemInstruction: systemInstruction,
      thinkingConfig: {
        thinkingBudget: thinkingBudget, 
      },
    }
  });

  let fullText = "";
  for await (const chunk of responseStream) {
    const chunkText = chunk.text;
    if (chunkText) {
      fullText += chunkText;
      if (onStream) onStream(chunkText);
    }
  }
  return fullText;
};

/**
 * Summarize a single chunk
 */
export const summarizeSingleChunk = async (
  text: string,
  promptConfig: PromptConfig,
  chunkIndex: number,
  totalChunks: number,
  onStream?: (text: string) => void
): Promise<string> => {
  const prompt = formatUserPrompt(promptConfig.user, totalChunks > 1, chunkIndex, totalChunks);
  // Force disable thinking for summarization to get detailed event logs
  return await callGemini(text, promptConfig.system, prompt, onStream, { useThinking: false });
};

/**
 * Main analysis function
 * @param text The raw text of the novel
 * @param type The type of analysis
 * @param promptConfig The prompt configuration
 * @param processedContext Optional pre-processed summary to use instead of raw text (saves tokens)
 * @param onStream Callback for streaming
 */
export const analyzeNovelText = async (
  text: string, 
  type: AnalysisType,
  promptConfig: PromptConfig,
  processedContext?: string,
  onStream?: (chunkText: string) => void
): Promise<string> => {
  try {
    let accumulatedResult = "";
    
    const handleStream = (newContent: string) => {
      accumulatedResult += newContent;
      if (onStream) {
        onStream(accumulatedResult);
      }
    };

    // --- INPUT VALIDATION & DEBUG PREVIEW ---
    // If using raw text (not context), validate it
    if (!processedContext && (!text || text.trim().length === 0)) {
        throw new Error("æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è¯»å–ã€‚è¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºæ”¯æŒçš„æ ¼å¼ (TXT/MD)ã€‚");
    }

    if (!processedContext) {
      // Print the first 50 characters to the stream so the user can verify if it's garbled
      const preview = text.slice(0, 50).replace(/\n/g, ' ');
      handleStream(`*ç³»ç»Ÿè¯Šæ–­: å·²è¯»å–æ–‡æœ¬ ${text.length} å­—ç¬¦ã€‚å‰ 50 å­—é¢„è§ˆ:ã€Œ${preview} ...ã€*\n*å¦‚æœä¸Šæ–¹é¢„è§ˆæ˜¾ç¤ºä¹±ç ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶ç¼–ç ã€‚*\n\n---\n\n`);
    }

    // STRATEGY 0: Use Processed Context (If available and applicable)
    // Style analysis MUST use raw text to detect wording nuances.
    // Summary agent (itself) MUST use raw text.
    // Others can benefit from the condensed summary.
    if (processedContext && type !== 'style' && type !== 'summary') {
      handleStream(`*âš¡ å·²å¯ç”¨ã€å…¨ä¹¦é€Ÿè¯»æƒ…æŠ¥ã€‘ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå¤§å¹…èŠ‚çœ Token å¹¶æé«˜åˆ†æèšç„¦åº¦...*\n\n---\n\n`);
      
      const prompt = formatUserPrompt(promptConfig.user, false);
      // We assume the summary fits in one context window comfortably
      await callGemini(processedContext, promptConfig.system, prompt, (chunk) => handleStream(chunk));
      return accumulatedResult;
    }

    // STRATEGY 1: Style Analysis (Sampling)
    if (type === 'style') {
      const sampledText = sampleTextForStyle(text);
      const prompt = formatUserPrompt(promptConfig.user, false);
      
      if (text.length > TARGET_CHUNK_SIZE) {
         handleStream(`*æ³¨ï¼šç”±äºæ–‡ä»¶è¿‡å¤§ï¼Œå·²è‡ªåŠ¨æ™ºèƒ½æˆªå–ã€å¼€å¤´ã€‘ã€ã€ä¸­é—´ã€‘ã€ã€ç»“å°¾ã€‘ä¸‰éƒ¨åˆ†æ ·æœ¬è¿›è¡Œç»¼åˆé£æ ¼åˆ†æï¼ˆè‡ªåŠ¨æ ¡å‡†å¥å­è¾¹ç•Œï¼‰...*\n\n---\n\n`);
      }
      
      await callGemini(sampledText, promptConfig.system, prompt, (chunk) => handleStream(chunk));
      return accumulatedResult;
    }

    // STRATEGY 2: Smart Chunking (Outline, Settings, Relationships, Theme, PlotHoles, Summary)
    const chunks = createSmartChunks(text);
    
    if (chunks.length === 1) {
      const prompt = formatUserPrompt(promptConfig.user, false);
      // Disable thinking for single-chunk summary as well
      const useThinking = type !== 'summary';
      await callGemini(chunks[0], promptConfig.system, prompt, (chunk) => handleStream(chunk), { useThinking });
    } else {
      handleStream(`*æ£€æµ‹åˆ°è¶…é•¿æ–‡æœ¬ï¼Œå·²æ™ºèƒ½åˆ†å‰²ä¸º ${chunks.length} ä¸ªè¯­ä¹‰å®Œæ•´çš„ç‰‡æ®µè¿›è¡Œæ·±åº¦åˆ†æ...*\n\n`);
      
      for (let i = 0; i < chunks.length; i++) {
        const header = `\n\n### ğŸ“œ ç¬¬ ${i + 1} éƒ¨åˆ† (å…± ${chunks.length} éƒ¨åˆ†)\n\n`;
        handleStream(header);
        
        const prompt = formatUserPrompt(promptConfig.user, true, i, chunks.length);
        // Disable thinking for summary chunks
        const useThinking = type !== 'summary';
        await callGemini(chunks[i], promptConfig.system, prompt, (chunk) => handleStream(chunk), { useThinking });
        
        handleStream(`\n\n---\n`);
      }

      // Final Summary Pass for Outline, Theme, AND Summary Agent itself
      if ((type === 'outline' || type === 'theme' || type === 'summary') && accumulatedResult.length < 200000) {
          let summaryPrompt = "";
          let summaryHeader = "";
          
          if (type === 'outline') {
            summaryHeader = `\n\n### ğŸ å…¨ä¹¦ç»“æ„æ€»ç»“\n\n*æ­£åœ¨åŸºäºä»¥ä¸Šåˆ†æ®µå¤§çº²ç”Ÿæˆå…¨ä¹¦æ•…äº‹å¼§çº¿æ€»ç»“...*\n\n`;
            summaryPrompt = "åŸºäºä»¥ä¸Šåˆ†æçš„æ‰€æœ‰åˆ†æ®µå¤§çº²ï¼Œè¯·æ€»ç»“å…¨ä¹¦çš„æ•…äº‹ä¸»çº¿ã€æ ¸å¿ƒçŸ›ç›¾æ¼”å˜ä»¥åŠæœ€ç»ˆç»“å±€ï¼ˆå¦‚æœåŒ…å«ï¼‰ã€‚è¯·ç”¨æœ€ç²¾ç‚¼çš„è¯­è¨€æ¢³ç†å‡ºä¸€ä¸ªâ€˜èµ·æ‰¿è½¬åˆâ€™çš„æ•´ä½“ç»“æ„ã€‚";
          } else if (type === 'theme') {
            summaryHeader = `\n\n### ğŸ æ ¸å¿ƒä¸»æ—¨å‡å\n\n*æ­£åœ¨ç»¼åˆåˆ†æå…¨ä¹¦çš„æ·±å±‚å¯“æ„...*\n\n`;
            summaryPrompt = "åŸºäºä»¥ä¸Šå„éƒ¨åˆ†çš„ä¸»é¢˜åˆ†æï¼Œè¯·æç‚¼è¿™æœ¬ä¹¦æœ€æ ¸å¿ƒçš„è¿™ä¸€ä¸ªâ€˜çµé­‚â€™ã€‚ä½œè€…åˆ°åº•æƒ³é€šè¿‡è¿™ä¸ªæ•…äº‹è¡¨è¾¾ä»€ä¹ˆï¼Ÿæ˜¯å…³äºäººæ€§çš„æŸç§æ´å¯Ÿï¼Œè¿˜æ˜¯å¯¹æŸç§ç¤¾ä¼šç°è±¡çš„éšå–»ï¼Ÿ";
          } else if (type === 'summary') {
            summaryHeader = "";
            summaryPrompt = ""; 
          }
          
          if (summaryPrompt) {
             handleStream(summaryHeader);
             // Enable thinking for the final meta-analysis
             await callGemini(accumulatedResult, "ä½ æ˜¯ä¸€ä½å–„äºæ€»ç»“çš„æ–‡å­¦ä¸»ç¼–ã€‚", summaryPrompt, (chunk) => handleStream(chunk), { useThinking: true });
          }
      }
    }

    return accumulatedResult;

  } catch (error) {
    console.error(`Error analyzing ${type}:`, error);
    if (error instanceof Error && error.message.includes("token")) {
        throw new Error("æ–‡æœ¬è¿‡é•¿æˆ–Tokenå¯†åº¦è¿‡é«˜ï¼Œå»ºè®®æ£€æŸ¥æ–‡ä»¶æ ¼å¼ã€‚å·²å°è¯•æ™ºèƒ½åˆ†ç‰‡ï¼Œä½†å•ç‰‡ä»è¶…å‡ºæ¨¡å‹é™åˆ¶ã€‚");
    }
    throw error;
  }
};
